:doctype: book
:icons: font
:hide-uri-scheme:

= Demo Debezium - Change Data Capture
:source-highlighter: rouge

For our demo, we are going to listen for changes on a sales order. The specific change we are interested in, is when an order is approved.

Since Odoo does not emit any events, we have to capture the change on the database level.

We are going to setup Debezium to capture the changes on the table sale_order.

Debezium configuration require two distinct areas. 

First we have to prepare the database to publish, or capture logs, depending on the RDBMS in question. For our instance, we are going to use the PogresSQL under Odoo.

Secondly, we have to configure the Kafka Connector to enable the events to be published to a topic in Kafka.

== Postgres Configuration

We have to configure the database engine configuration, and also apply some changes on a schema level, through sql interaction.

For the database configuration:

- From *Admin* perspective, select *Projects*, from the *Home*
- Click on the *odoo* project
- In the navigation panel, select *Pods*, under *Workloads*
- On the list of pods, click on the pod named *db-..*
- With pod information open, click on the *Terminal* tab, to open a terminal session

Now that we have access to the database host pod, we can start doing the configuration.

.Postgresql Terminal
image::postgresql-terminal.png[]

On the terminal 
- Click on *Expand* to open a full window  
- cd /var/lib/pgsql/data/userdata
- vi postgress.conf

Apply the changes as follow at the the bottom of the file, *#WAL / Replication* paragraph should be added:

.Changes to postgresql.conf
[code,,highlith=5-7]
----
#------------------------------------------------------------------------------
# CUSTOMIZED OPTIONS
#------------------------------------------------------------------------------

# Add settings for extensions here

# WAL / replication
wal_level = logical
max_wal_senders = 3

# Custom OpenShift configuration:
include '/var/lib/pgsql/openshift-custom-postgresql.conf'
----

- *Collapse* to get back to pod information panel
- Restart pod, using *Actions*, *Delete Pod*, to force the Postgresql to restart with new configuration.

After the pod has restarted, we have to enable the odoo database with replication.

- Again, following steps above, to get the terminal of the *db-..* pod.
- In the terminal, execute the following:

[code,sql]
----
psql -U postgres        
psql (10.21)
Type "help" for help.

postgres=# create role debezium replication login;
CREATE ROLE
postgres=# grant debezium to odoo;
GRANT ROLE
postgres=# ALTER USER odoo WITH SUPERUSER;
ALTER ROLE
postgres=#/q
----

== Configuring Kafka Connect

Kafka Connect is the service to which a Kafka Connector is deployed to. The connector is build by OpenShift's S2I with all the relevant libraries required, as specified in the KafkaConnect resource.

In order to use Debezium, we have to include the relevant libraries into the Kafka Connect. We are also going to deploy the service registry libraries.

To create a Kafka Connect configuration:

We have to create a target ImageStream for the output of the build of our connector, before we start the build.

- Click on the *+* at the top next to your username.
- Paste the following YAML, to create ImageStream

.YAML for ImageStream
https://raw.githubusercontent.com/pmalan-rh/EDAOpenShift-code/main/debezium/debezium-streams-connect.yaml

[code]
----
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: debezium-streams-connect
  namespace: amq-streams
----

- From the *Administrator* perspective, navigate to *Projects*, under *Home*
- Select the *amq-streams* from the project list
- Click on *Installed Operators*, and click on *Red Hat Integration - AQM Streams*
- Click on the *Create Instance* on the *Kafka Connect* panel
- Click on the *YAML view* on the *Configure via* option
- Change the YAML to reflect the following:

.YAML for Kafka Connect
https://raw.githubusercontent.com/pmalan-rh/EDAOpenShift-code/main/debezium/my-connect-cluster.yaml

.Kafka Connect YAML
[code]
----
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  namespace: amq-streams
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  config:
    group.id: connect-cluster
    offset.storage.topic: connect-cluster-offsets
    config.storage.topic: connect-cluster-configs
    status.storage.topic: connect-cluster-status
    config.storage.replication.factor: -1
    offset.storage.replication.factor: -1
    status.storage.replication.factor: -1
    key.converter: io.apicurio.registry.utils.converter.AvroConverter
    key.converter.schema.registry.url: http://service-registry-service.service-registry.svc.cluster.local:8080/api
    key.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy
    key.converter.apicurio.registry.as-confluent: true
    key.converter.apicurio.registry.auto-register: true
    value.converter: io.apicurio.registry.utils.converter.AvroConverter
    key.converter.schema.registry.url: http://service-registry-service.service-registry.svc.cluster.local:8080/api
    value.converter.apicurio.registry.global-id: io.apicurio.registry.utils.serde.strategy.AutoRegisterIdStrategy
  tls:
    trustedCertificates:
      - secretName: my-cluster-cluster-ca-cert
        certificate: ca.crt
  version: 3.1.0
  build:
    output:
      type: imagestream
      image: debezium-streams-connect:lastest
    plugins:
      - name: debezium-connector-postgresql
        artifacts:
          - type: zip
            url: https://maven.repository.redhat.com/ga/io/debezium/debezium-connector-postgres/1.9.5.Final-redhat-00001/debezium-connector-postgres-1.9.5.Final-redhat-00001-plugin.zip
          - type: zip
            url: https://maven.repository.redhat.com/ga/io/apicurio/apicurio-registry-distro-connect-converter/1.2.2.Final-redhat-00005/apicurio-registry-distro-connect-converter-1.2.2.Final-redhat-00005-converter.zip
          - type: jar
            url: https://maven.repository.redhat.com/ga/io/apicurio/apicurio-registry-serdes-jsonschema-serde/2.0.0.Final-redhat-00005/apicurio-registry-serdes-jsonschema-serde-2.0.0.Final-redhat-00005.jar
        
  replicas: 1
  bootstrapServers: 'my-cluster-kafka-bootstrap:9093'
----

[TIP]
.Latest Releases for Debezium
====
The get the latest Debezium plugins, you can browse to the following URL, and search for specific plugins.

https://maven.repository.redhat.com/ga/io/debezium/

For the APICurio Registry converter:

https://maven.repository.redhat.com/ga/io/apicurio/apicurio-registry-distro-connect-converter/

====

=== Verify Build

To verify that the build was successful, from *Administrator* perspective, go to *Builds* under heading *Builds*.

You should see a *complete* build if configuration applied correctly.

.Completed Build
image::amq-connect-build.png[]

== Configuring Kafka Connector

The Kafka Connector is responsible to define the database connection and relevant schema elements we are interested in capturing change events from. This configuration is fed into the Kafka Connect to start capturing of events.

[code]
----
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  labels:
    strimzi.io/cluster: my-connect-cluster
  name: sales-connector-postgresql 
spec:
  class: io.debezium.connector.postgresql.PostgresConnector 
  tasksMax: 1 
  config:  
    database.history.kafka.bootstrap.servers: 'my-cluster-kafka-bootstrap.amq-streams.svc:9092'
    database.history.kafka.topic: schema-changes.sales
    database.hostname: db.odoo.svc.cluster.local 
    database.port: 5432   
    database.user: odoo 
    database.password: odoo  
    database.dbname: odoo 
    database.server.name: sales_connector_postgresql 
    database.include.list: public.sale_order 
    plugin.name: pgoutput
----

== Testing Debezium

== Peek into Kafka

UI for Apache Kafka is a third=party tool to see events in Kafka.

Steps for eploying of UI for Apache Kafka:

- From *Administrator* perspective, click *Projects* under *Home* section
- Click *amq_streams* on the list of projects
- Change the perspective to *Developer*
- Right click on *Topology Map*, *Add to Project*, and click on *Container Image*
- Specify *provectuslabs/kafka-ui:latest* for *Image name from external registry*
- Leave the rest defaults, and click on *Deployment* at the bottom in the *Click on the names to access ..* sentence
- Under *Environment Variables, add the following values:

Name: *KAFKA_CLUSTERS_0_NAME* value *amq-streams*

Name: *KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS* value *my-cluster-kafka-bootstrap.amq-streams.svc:9092*

- Click on *Create*

Next we have to get the URL to application:

- Click on *Project*
- Click on *Route* on the *Inventory* panel
- Click on the *Location*, next to *kafka-ui*

UI for Apache Kafka, will display a list of options, select *Topics*, and *odoo.public.sale_order*

- select the *Messages* tab
- On the far right hand side, change the dropdown *Oldest First* to *Live Mode*

In the next section, we are going to create a Kafka message, by update a sales order.

=== Odoo Update

Login into Odoo, refer to Odoo configuration to get login link.

After login go to *Sales*

image::odoo-sales-menu.png[]

To update a quote:

- Click on the first *quote* in the table
- Click on *Edit*
- Update the *Payment Terms*, to a new value
- Click on *Save*

Switching back to UI for Apache Kafka, you will a message similar to the following, after clicking on the + sign next to new message:

image::ui-kafka.png[]

UI for Apache Kafka does not decode the message correctly, since it does not understand the associated AVRO schema, but at least we can confirm that Debezium is doing it's job as expected.

Last check is to see if the schema was registered in the service registry.

Go to the Service Registry home page:

- You would be able to see the auto registered AVRO schemas for our defined Debezium events, as illustrated in the follwong image.

.Registered Schemas in Service Registry
image::service-registry-schemas.png[]
